---
title: "model_basic_PCA"
author: "Niklas Pawelzik"
date: "2024-10-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## import data 

```{r}
# Define the input path for the processed files
input_path <- "./cleaned_data/"  

# Import wide dataframes 
df_WK_kommunen_training_wide <- read.csv(file = paste0(input_path, "df_WK_kommunen_training_wide.csv"))
df_WK_kommunen_finance_wide <- read.csv(file = paste0(input_path, "df_WK_kommunen_finance_wide.csv"))
df_WK_kommunen_demographics_wide <- read.csv(file = paste0(input_path, "df_WK_kommunen_demographics_wide.csv"))
df_WK_kommunen_employment_wide <- read.csv(file = paste0(input_path, "df_WK_kommunen_employment_wide.csv"))
df_WK_kommunen_social_wide <- read.csv(file = paste0(input_path, "df_WK_kommunen_social_wide.csv"))

# Import long dataframes from CSV files
df_WK_kommunen_training_long <- read.csv(file = paste0(input_path, "df_WK_kommunen_training_long.csv"))
df_WK_kommunen_finance_long <- read.csv(file = paste0(input_path, "df_WK_kommunen_finance_long.csv"))
df_WK_kommunen_demographics_long <- read.csv(file = paste0(input_path, "df_WK_kommunen_demographics_long.csv"))
df_WK_kommunen_employment_long <- read.csv(file = paste0(input_path, "df_WK_kommunen_employment_long.csv"))
df_WK_kommunen_social_long <- read.csv(file = paste0(input_path, "df_WK_kommunen_social_long.csv"))
```

```{r}
# List of all dataframes to process
dfs <- list(
  df_social = df_social,
  df_demographics = df_demographics, 
  df_employment = df_employment, 
  df_training = df_training,
  df_finance = df_finance
  )

# Apply the cleaning function to all dataframes
cleaned_dfs <- lapply(dfs, process_and_clean_WK_file)

# Extract individual cleaned dataframes from the list
df_WK_kommunen_social_long <- cleaned_dfs$df_social
df_WK_kommunen_demographics_long <- cleaned_dfs$df_demographics
df_WK_kommunen_employment_long <- cleaned_dfs$df_employment
df_WK_kommunen_training_long <- cleaned_dfs$df_training
df_WK_kommunen_finance_long <- cleaned_dfs$df_finance
```

```{r functions are in WK API document}
# List of all dataframes to process
dfs_long <- list(
  df_WK_kommunen_social_long = df_WK_kommunen_social_long,
  df_WK_kommunen_demographics_long = df_WK_kommunen_demographics_long, 
  df_WK_kommunen_employment_long = df_WK_kommunen_employment_long, 
  df_WK_kommunen_training_long = df_WK_kommunen_training_long, 
  df_WK_kommunen_finance_long = df_WK_kommunen_finance_long
  )

# Apply the cleaning function to all dataframes
wide_dfs <- lapply(dfs_long, convert_to_wide)

# Extract individual cleaned dataframes from the list
df_WK_kommunen_social_wide <- wide_dfs$df_WK_kommunen_social_long
df_WK_kommunen_demographics_wide <- wide_dfs$df_WK_kommunen_demographics_long
df_WK_kommunen_employment_wide <- wide_dfs$df_WK_kommunen_employment_long
df_WK_kommunen_training_wide <- wide_dfs$df_WK_kommunen_training_long
df_WK_kommunen_finance_wide <- wide_dfs$df_WK_kommunen_finance_long
```

## Getting to know the data, Kommunen (finance)

```{r}
colSums(is.na(df_WK_kommunen_employment_wide))
```

```{r}
colSums(is.na(df_WK_kommunen_finance_wide))
```

```{r}
plot_na_matrix <- function(df) {     
  # Preparing the dataframe for heatmaps     
  df_heat <- df %>%        
    pivot_longer(cols = everything(),
                 names_to = "x") %>%
    group_by(x) %>%
    mutate(y = row_number())
  # Ensuring the order of columns is kept as it is
  df_heat <- df_heat %>%
    ungroup() %>%
    mutate(x = factor(x,levels = colnames(df)))
  # Plotting data
  g <- ggplot(data = df_heat, aes(x=x, y=y, fill=value)) +
    geom_tile() +
    theme(legend.position = "none",
          axis.title.y=element_blank(),
          axis.text.y =element_blank(),
          axis.ticks.y=element_blank(),
          axis.title.x=element_blank(),
          axis.text.x = element_text(angle = 90, hjust = 1))
     # Returning the plot
    g 
}
```

```{r}
df_na_kommunen <- purrr::map_df(df_WK_kommunen_finance_wide, function(x) as.numeric(is.na(x))) 
df_na_kommunen_heat <- df_na_kommunen %>%
  pivot_longer(cols = everything(),
               names_to = "x") %>%
  group_by(x) %>%
  mutate(y = row_number())

plot_na_matrix(df_na_kommunen[1:20, ])
```

```{r}
df_WK_kommunen_finance_wide %>%
  dplyr::select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs")
```

```{r}
df_WK_kommunen_finance_wide %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```

### Doing PCA (dirty,  finance)

#### Clean the Dataset (dirty, finance)

```{r focus on columsn with few NAs, remove remaining NAs}
PCA_prep_finance_no_NA <- df_WK_kommunen_finance_wide %>%
  dplyr::select(
    - 'Hebesatz Grundsteuer B (v.H.)',
    - 'Hebesatz Gewerbesteuer (v.H.)',
    - 'Jugendhilfe (Euro je Einwohner:in)',
    - 'Investitionskredite % zum Vorjahr (%)',
    - 'Liquiditätskredite % zum Vorjahr (%)',
    - 'Steuereinnahmen pro Einwohner:in (Euro je Einwohner:in)',
    - 'Demografietyp'
  ) %>%
  na.omit()

colSums(is.na(PCA_prep_finance_no_NA))

```


```{r cap outliers of first and fourth quantile}
# Get only the numeric columns from the dataset
numeric_columns <- sapply(PCA_prep_finance_no_NA, is.numeric)

# Create a copy of the dataset to store the cleaned data
PCA_prep_finance_no_NA_no_outlier <- PCA_prep_finance_no_NA

# Loop over each numeric column
for (col in names(PCA_prep_finance_no_NA_no_outlier)[numeric_columns]) {
  
  # Calculate the IQR for the column
  Q1 <- quantile(PCA_prep_finance_no_NA_no_outlier[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(PCA_prep_finance_no_NA_no_outlier[[col]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Define the lower and upper bounds for outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Cap outliers at the lower and upper bounds
  PCA_prep_finance_no_NA_no_outlier[[col]][PCA_prep_finance_no_NA_no_outlier[[col]] < lower_bound] <- lower_bound
  PCA_prep_finance_no_NA_no_outlier[[col]][PCA_prep_finance_no_NA_no_outlier[[col]] > upper_bound] <- upper_bound
}

```

#### Run PCA (dirty, finance)

```{r Select Predictor Variables for the PCA, standardize the data}
PCA_PVs_selected_columns <- c(
    'Einzahlungen lfd. Verwaltung (Euro je Einwohner:in)',
    'Finanzeinzahlungen (Euro je Einwohner:in)',
    'Finanzsaldo (Euro je Einwohner:in)',
    'Grundsteuer B (Euro je Einwohner:in)',
    'Umlage an Gemeindeverbände (Euro je Einwohner:in)',
    'Verschuldung im Kernhaushalt (Euro je Einwohner:in)'
)

PCA_prep_finance_standardized_data <- scale(PCA_prep_finance_no_NA_no_outlier[, PCA_PVs_selected_columns])
```

```{r needs more thought. Do I kick out variables that are to closely related? Porbably not, the point of PCA is getting rid of correlation}
PCA_prep_finance_standardized_data %>%
  data.frame() %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```

```{r}
# Perform PCA
pca_result <- prcomp(PCA_prep_finance_standardized_data, center = TRUE, scale = TRUE)

# View PCA summary to see variance explained by each component
summary(pca_result)


# Extract the explained variance for each component
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)

# Create a scree plot to visualize how much variance each component explains
screeplot(pca_result, type = "lines", main = "Scree Plot")

# Assuming you've stored your PCA result in pca_result
loadings <- pca_result$rotation

# View the loadings
print(loadings)

# Biplot to visualize the loadings and the PCs
biplot(pca_result, scale = 0)
```


## Getting to know the data, Kommunen (demographics)

```{r}
colSums(is.na(df_WK_kommunen_demographics_wide))
```

```{r}
df_na_kommunen <- purrr::map_df(df_WK_kommunen_demographics_wide, function(x) as.numeric(is.na(x))) 
df_na_kommunen_heat <- df_na_kommunen %>%
  pivot_longer(cols = everything(),
               names_to = "x") %>%
  group_by(x) %>%
  mutate(y = row_number())

plot_na_matrix(df_na_kommunen[1:20, ])
```


```{r}
df_WK_kommunen_demographics_wide %>%
  dplyr::select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs")
```

```{r}
df_WK_kommunen_demographics_wide %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```


### Doing PCA (dirty, demographics)

#### Clean the Dataset (dirty, demographics)

```{r focus on columsn with few NAs, remove remaining NAs}
PCA_prep_demographics_no_NA <- df_WK_kommunen_demographics_wide %>%
  dplyr::select(
    - 'Bevölkerungsentwicklung seit 2011 (%)',
    - 'Bevölkerungsentwicklung über die letzten 5 Jahre (%)',
    - 'Vorzeitige Sterblichkeit - Frauen (Todesfälle je 1.000 Einwohner:innen)',
    - 'Vorzeitige Sterblichkeit - Männer (Todesfälle je 1.000 Einwohner:innen)',
    - 'Zuzüge (je 1.000 Einwohner:innen)',
    - 'Fortzüge (je 1.000 Einwohner:innen)',
    - 'Wanderungssaldo (je 1.000 Einwohner:innen)',
    - 'Familienwanderung (je 1.000 Einwohner:innen)',
    - 'Bildungswanderung (je 1.000 Einwohner:innen)',
    - 'Wanderung zu Beginn der 2. Lebenshälfte (je 1.000 Einwohner:innen)',
    - 'Alterswanderung (je 1.000 Einwohner:innen)'
  ) %>%
  na.omit()

colSums(is.na(PCA_prep_demographics_no_NA))

```


```{r cap outliers of first and fourth quantile}
# Get only the numeric columns from the dataset
numeric_columns <- sapply(PCA_prep_demographics_no_NA, is.numeric)

# Create a copy of the dataset to store the cleaned data
PCA_prep_demographics_no_NA_no_outlier <- PCA_prep_demographics_no_NA

# Loop over each numeric column
for (col in names(PCA_prep_demographics_no_NA_no_outlier)[numeric_columns]) {
  
  # Calculate the IQR for the column
  Q1 <- quantile(PCA_prep_demographics_no_NA_no_outlier[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(PCA_prep_demographics_no_NA_no_outlier[[col]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Define the lower and upper bounds for outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Cap outliers at the lower and upper bounds
  PCA_prep_demographics_no_NA_no_outlier[[col]][PCA_prep_demographics_no_NA_no_outlier[[col]] < lower_bound] <- lower_bound
  PCA_prep_demographics_no_NA_no_outlier[[col]][PCA_prep_demographics_no_NA_no_outlier[[col]] > upper_bound] <- upper_bound
}

```


#### Run PCA (dirty, demographics)

```{r Select Predictor Variables for the PCA, standardize the data}
PCA_PVs_selected_columns_demographics <- c(
    'Anteil 65- bis 79-Jährige (%)',
    'Anteil Elternjahrgänge (%)',
    'Anteil unter 18-Jährige (%)',
    'Geburten (je 1.000 Einwohner:innen)',
    'Altenquotient (ab 65-Jährige je 100 Pers. der AG 20-64)',
    'Jugendquotient (unter 20-Jährige je 100 Pers. der AG 20-64)',
    'Gesamtquotient (unter 20-/ab 65-Jährige je 100 Pers. der AG 20-64)',
    'Natürlicher Saldo (je 1.000 Einwohner:innen)'
)

PCA_prep_demographics_standardized_data <- scale(PCA_prep_demographics_no_NA_no_outlier[, PCA_PVs_selected_columns_demographics])
```

```{r needs more thought. Do I kick out variables that are to closely related? Porbably not, the point of PCA is getting rid of correlation}
PCA_prep_demographics_standardized_data %>%
  data.frame() %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```

```{r}
# Perform PCA
pca_result_demographics <- prcomp(PCA_prep_demographics_standardized_data, center = TRUE, scale = TRUE)

# View PCA summary to see variance explained by each component
summary(pca_result_demographics)


# Extract the explained variance for each component
explained_variance_demographics <- pca_result_demographics$sdev^2 / sum(pca_result_demographics$sdev^2)

# Create a scree plot to visualize how much variance each component explains
screeplot(pca_result_demographics, type = "lines", main = "Scree Plot")

# Assuming you've stored your PCA result in pca_result
loadings_demographics <- pca_result_demographics$rotation

# View the loadings
print(loadings_demographics)

# Biplot to visualize the loadings and the PCs
biplot(pca_result_demographics, scale = 0)
```



## Getting to know the data, Kommunen (social)

```{r}
colSums(is.na(df_WK_kommunen_social_wide))
```

```{r}
df_na_kommunen <- purrr::map_df(df_WK_kommunen_social_wide, function(x) as.numeric(is.na(x))) 
df_na_kommunen_heat <- df_na_kommunen %>%
  pivot_longer(cols = everything(),
               names_to = "x") %>%
  group_by(x) %>%
  mutate(y = row_number())

plot_na_matrix(df_na_kommunen[1:20, ])
```


```{r}
df_WK_kommunen_social_wide %>%
  dplyr::select(where(is.numeric)) %>%
  cor(use = "pairwise.complete.obs")
```

```{r}
df_WK_kommunen_social_wide %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```

### Doing PCA (dirty, social)

#### Clean the Dataset (dirty, social)

```{r focus on columns with few NAs, remove remaining NAs}
PCA_prep_social_no_NA <- df_WK_kommunen_social_wide %>%
  dplyr::select(
    - 'Haushalte mit niedrigem Einkommen (%)',
    - 'Haushalte mit mittlerem Einkommen (%)',
    - 'Haushalte mit hohem Einkommen (%)',
    - 'Arbeitslose an den SvB (%)',
    - 'Arbeitslose an den ausländischen SvB (%)',
    - 'Kinderarmut (%)',
    - 'Jugendarmut (%)',
    - 'Altersarmut (%)',
    - 'Breitbandversorgung - Private Haushalte (%)'
  ) %>%
  na.omit()

colSums(is.na(PCA_prep_social_no_NA))

```

```{r cap outliers of first and fourth quantile}
# Get only the numeric columns from the dataset
numeric_columns <- sapply(PCA_prep_social_no_NA, is.numeric)

# Create a copy of the dataset to store the cleaned data
PCA_prep_social_no_NA_no_outlier <- PCA_prep_social_no_NA

# Loop over each numeric column
for (col in names(PCA_prep_social_no_NA_no_outlier)[numeric_columns]) {
  
  # Calculate the IQR for the column
  Q1 <- quantile(PCA_prep_social_no_NA_no_outlier[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(PCA_prep_social_no_NA_no_outlier[[col]], 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  
  # Define the lower and upper bounds for outliers
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  
  # Cap outliers at the lower and upper bounds
  PCA_prep_social_no_NA_no_outlier[[col]][PCA_prep_social_no_NA_no_outlier[[col]] < lower_bound] <- lower_bound
  PCA_prep_social_no_NA_no_outlier[[col]][PCA_prep_social_no_NA_no_outlier[[col]] > upper_bound] <- upper_bound
}

```


#### Run PCA (dirty, social)

```{r Select Predictor Variables for the PCA, standardize the data}
PCA_PVs_selected_columns_social <- c(
  'Einpersonen-Haushalte (%)',
  'Haushalte mit Kindern (%)',
  'Wohnfläche pro Person (m²)',
  'Wohnungen in Ein-/Zweifamilienhäusern (%)'
)

PCA_prep_social_standardized_data <- scale(PCA_prep_social_no_NA_no_outlier[, PCA_PVs_selected_columns_social])
```

```{r needs more thought. Do I kick out variables that are to closely related? Porbably not, the point of PCA is getting rid of correlation}
PCA_prep_social_standardized_data %>%
  data.frame() %>%
  dplyr::select(where(is.numeric)) %>%
  ggcorr(, 
       label = TRUE,    # Show correlation coefficients
       label_round = 2, # Round coefficients to two decimal places
       limits = c(-1, 1), label_size = 2 # Correlation scale limits
)
```

```{r}
# Perform PCA
pca_result_social <- prcomp(PCA_prep_social_standardized_data, center = TRUE, scale = TRUE)

# View PCA summary to see variance explained by each component
summary(pca_result_social)


# Extract the explained variance for each component
explained_variance_social <- pca_result_social$sdev^2 / sum(pca_result_social$sdev^2)

# Create a scree plot to visualize how much variance each component explains
screeplot(pca_result_social, type = "lines", main = "Scree Plot")

# Assuming you've stored your PCA result in pca_result
loadings_social <- pca_result_social$rotation

# View the loadings
print(loadings_social)

# Biplot to visualize the loadings and the PCs
biplot(pca_result_social, scale = 0)
```

## Only Dependent Variable
```{r}
dataframe_dependent_variables <- df_WK_kommunen_employment_wide %>%
  dplyr::select(
    "Kommune", "GKZ", "ARS", "Bundesland", "Landkreis", "Demografietyp", "Year",
    "Beschäftigungsquote (%)",
    "Frauenbeschäftigungsquote (%)",
    "Beschäftigungsquote 55- bis 64-Jährige (%)"
  ) %>%
  rename(
    employment_level = "Beschäftigungsquote (%)",
    employment_level_female = "Frauenbeschäftigungsquote (%)",
    ) %>%
# Interpolate NA values within each group
  group_by(ARS) %>%
  mutate(
    employment_level = zoo::na.approx(employment_level, na.rm = FALSE, maxgap = 3),
    employment_level_female = zoo::na.approx(employment_level_female, na.rm = FALSE, maxgap = 3),
  ) %>%
  ungroup() %>%
  filter(
    Year != 2021 & Year != 2022
    ) %>%
# Interpolate NA values within each group (pre-post filtering for years might have impact on leading/trailing values and maxgap trigger)
  group_by(ARS) %>%
  mutate(
    employment_level = zoo::na.approx(employment_level, na.rm = FALSE, maxgap = 3),
    employment_level_female = zoo::na.approx(employment_level_female, na.rm = FALSE, maxgap = 3),
  ) %>%
  ungroup()

```

## Selection of Covariates
```{r}
df_count_population <- df_WK_kommunen_demographics_wide %>%
  filter(
    Year == 2009
    )

df_count_population$`Bevölkerung (Anzahl)` %>%
  sum(na.rm = TRUE)
```

```{r}
df_covariates_combined <- df_WK_kommunen_finance_wide %>%
  filter(Year == 2009) %>%
  select(
    Kommune, GKZ, ARS, Bundesland, Landkreis, Demografietyp, Year,
    "Verschuldung im Kernhaushalt (Euro je Einwohner:in)",
    "Steuereinnahmen pro Einwohner:in (Euro je Einwohner:in)",
    "Finanzmittelsaldo (Euro je Einwohner:in)"
  ) %>%
  rename(
    core_budget_debt_per_capita = "Verschuldung im Kernhaushalt (Euro je Einwohner:in)",
    tax_revenue_per_capita = "Steuereinnahmen pro Einwohner:in (Euro je Einwohner:in)",
    net_funds_balance_per_capita = "Finanzmittelsaldo (Euro je Einwohner:in)"
  ) %>%
  left_join(
    df_WK_kommunen_employment_wide %>%
      filter(Year == 2009) %>%
      select(
        Kommune, GKZ, ARS,
        "Hochqualifizierte am Wohnort (%)",
        "Geringfügig Beschäftigte (Wohnort) (je 1.000 Einwohner:innen)"
      ) %>%
      rename(
        highly_qualified_residents = "Hochqualifizierte am Wohnort (%)",
        marginal_employment_per_1000_residents = "Geringfügig Beschäftigte (Wohnort) (je 1.000 Einwohner:innen)"
      ),
    by = c("Kommune", "GKZ", "ARS")
  ) %>%
  left_join(
    df_WK_kommunen_social_wide %>%
      filter(Year == 2009) %>%
      select(
        Kommune, GKZ, ARS,
        "ALG II-Quote (%)",
        "Arbeitslose an den SvB unter 25 Jahren (%)"
      ) %>%
      rename(
        unemployment_benefit_ii_rate = "ALG II-Quote (%)",
        unemployed_sv_insured_under_25 = "Arbeitslose an den SvB unter 25 Jahren (%)"
      ),
    by = c("Kommune", "GKZ", "ARS")
  )

```

## Overall dataset
```{r}
df_WK_kommunen_employment_wide_only_DV <- df_WK_kommunen_employment_wide %>%
  dplyr::select(
    "Kommune", "GKZ", "ARS", "Bundesland", "Landkreis", "Demografietyp", "Year", "Beschäftigungsquote (%)", "Frauenbeschäftigungsquote (%)", "Beschäftigungsquote 55- bis 64-Jährige (%)"
  )

# List of dataframes to merge
dataframes <- list(
  df_WK_kommunen_demographics_wide,
  df_WK_kommunen_employment_wide_only_DV,
  df_WK_kommunen_finance_wide,
  df_WK_kommunen_social_wide
#  df_WK_kommunen_training_wide
)

# Merge dataframes iteratively using `reduce` and `full_join`
df_wide_complete <- reduce(dataframes, function(x, y) {
  full_join(x, y, by = c("Kommune", "GKZ", "ARS", "Bundesland", "Landkreis", "Demografietyp", "Year"))
})

# Rename the column and create a new column with the first 9 digits
df_wide_complete <- df_wide_complete %>%
  rename(ARS_original = ARS) %>% # Rename "ARS" to "ARS_original"
  mutate(
    ARS = substr(ARS_original, 1, 9) # Extract the first 9 digits
  )

# Filter to retain only rows without any NA values
df_wide_complete_no_na <- df_wide_complete %>%  
  dplyr::select(
    - 'Hebesatz Grundsteuer B (v.H.)',
    - 'Hebesatz Gewerbesteuer (v.H.)',
    - 'Jugendhilfe (Euro je Einwohner:in)',
    - 'Investitionskredite % zum Vorjahr (%)',
    - 'Liquiditätskredite % zum Vorjahr (%)',
    - 'Steuereinnahmen pro Einwohner:in (Euro je Einwohner:in)',
#    - 'Demografietyp',
    - 'Bevölkerungsentwicklung seit 2011 (%)',
    - 'Bevölkerungsentwicklung über die letzten 5 Jahre (%)',
    - 'Vorzeitige Sterblichkeit - Frauen (Todesfälle je 1.000 Einwohner:innen)',
    - 'Vorzeitige Sterblichkeit - Männer (Todesfälle je 1.000 Einwohner:innen)',
    - 'Zuzüge (je 1.000 Einwohner:innen)',
    - 'Fortzüge (je 1.000 Einwohner:innen)',
    - 'Wanderungssaldo (je 1.000 Einwohner:innen)',
    - 'Familienwanderung (je 1.000 Einwohner:innen)',
    - 'Bildungswanderung (je 1.000 Einwohner:innen)',
    - 'Wanderung zu Beginn der 2. Lebenshälfte (je 1.000 Einwohner:innen)',
    - 'Alterswanderung (je 1.000 Einwohner:innen)',
    - 'Haushalte mit niedrigem Einkommen (%)',
    - 'Haushalte mit mittlerem Einkommen (%)',
    - 'Haushalte mit hohem Einkommen (%)',
    - 'Arbeitslose an den SvB (%)',
    - 'Arbeitslose an den ausländischen SvB (%)',
    - 'Kinderarmut (%)',
    - 'Jugendarmut (%)',
    - 'Altersarmut (%)',
    - 'Breitbandversorgung - Private Haushalte (%)'
  ) %>%
  filter(complete.cases(.))

# Get the row counts
row_count <- nrow(df_wide_complete)
row_count_no_na <- nrow(df_wide_complete_no_na)

# Print the row count
print(paste("Number of rows:", row_count))
print(paste("Number of rows without any NA values:", row_count_no_na))
print(paste("Number of rows without any NA values demographics:",nrow(PCA_prep_demographics_no_NA)))
print(paste("Number of rows without any NA values finance:",nrow(PCA_prep_finance_no_NA)))
print(paste("Number of rows without any NA values social:",nrow(PCA_prep_social_no_NA)))
print(paste("Number of rows without any NA values employment:",nrow(df_WK_kommunen_employment_wide_only_DV)))
head(df_wide_complete)

```

```{r}
# Perform a left join to enrich df_wide_complete_no_na
df_wide_complete_no_na_hospital_closures <- df_wide_complete_no_na %>%
  left_join(
    df_hospital_closures_newspaper_confirmed_ARS %>%
      dplyr::select(ARS, closure_context_info, year_of_closure), # Select relevant columns
    by = "ARS" # Match based on ARS
  ) %>%
  # Rename the columns for consistency
  rename(
    confirmed_closure = closure_context_info,
    year_of_closure = year_of_closure
  )

# Convert data types for comparison
df_wide_complete_no_na_hospital_closures <- df_wide_complete_no_na_hospital_closures %>%
  mutate(
    Year = as.numeric(Year), # Convert Year from character to numeric
    year_of_closure = as.numeric(as.character(year_of_closure)) # Convert year_of_closure from factor to numeric
  )

# Create treated_unit and treatment_received columns
df_wide_complete_no_na_hospital_closures <- df_wide_complete_no_na_hospital_closures %>%
  mutate(
    # treated_unit: TRUE if year_of_closure is not NA, FALSE otherwise
    treated_unit = !is.na(year_of_closure),
    
    # treatment_received: TRUE if Year >= year_of_closure, FALSE otherwise
    treatment_received = ifelse(!is.na(year_of_closure) & Year >= year_of_closure, TRUE, FALSE)
  )


head(df_wide_complete_no_na_hospital_closures)
```

```{r}
# Join the shapefile with dataframe
shp_flächen_ebene_verwaltungsgemeinschaft_enriched <- shp_flächen_ebene_verwaltungsgemeinschaft %>%
    distinct(ARS, .keep_all = TRUE)  %>% # Keep only one row per ARS
  left_join(df_wide_complete_no_na_hospital_closures, by = "ARS") # Ensure "ARS" matches in both
```

```{r}
ggplot(data = shp_flächen_ebene_verwaltungsgemeinschaft_enriched) +
  geom_sf(aes(fill = treated_unit)) + # Use "treated_unit" to define fill colors
  theme_minimal() +
  labs(
    title = "Gemeinden in Deutschland: Treated Units",
    fill = "Treated Unit" # Legend title
  )
```


```{r visualization per year of closure, works but is very demanding}
# Create a layer for country borders
germany_border <- shp_flächen_ebene_verwaltungsgemeinschaft_enriched %>%
  summarise(geometry = st_union(geometry)) # Union all geometries for country borders


custom_colors <- c(
  "2023" = "darkcyan",
  "2022" = "darkblue",
  "2021" = "darkred",
  "2020" = "darkgreen",
  "2019" = "orange",
  "2018" = "purple",
  "2017" = "yellow",
  "2016" = "cyan",
  "2015" = "magenta",
  "2014" = "green",
  "2013" = "blue",
  "2012" = "red",
  "2010" = "chocolate"
)

# Include 'No Data' and 'Non-treated' colors
all_colors <- c(
  "No Data" = "white",     # White for No Data
  "Non-treated" = "grey",  # Grey for non-treated units
  custom_colors            # Your defined colors for treated years
)

# Updated ggplot code
ggplot() +
  geom_sf(
    data = shp_flächen_ebene_verwaltungsgemeinschaft_enriched,
    aes(
      fill = case_when(
        is.na(treated_unit) ~ "No Data",          # NA units: "No Data"
        treated_unit == FALSE ~ "Non-treated",    # Non-treated units: grey
        treated_unit == TRUE ~ as.character(year_of_closure) # Treated: color by year
      )
    ),
    color = case_when(
      is.na(shp_flächen_ebene_verwaltungsgemeinschaft_enriched$treated_unit) ~ NA, # No borders for NA regions
      TRUE ~ "black" # Borders for other regions
    ),
    size = 0.1 # Thin borders for regions
  ) +
  geom_sf(
    data = germany_border,
    fill = NA, # Transparent fill
    color = "black", # Country border color
    size = 0.5 # Thicker line for the country border
  ) +
  scale_fill_manual(
    values = all_colors, # Use the custom palette
    na.value = "white",  # White fill for NA
    guide = guide_legend(override.aes = list(size = 4)) # Ensure legend markers are large enough
  ) +
  theme_minimal() +
  labs(
    title = "Municipalities in Germany: Hospital Closures by Year",
    fill = "Year of Closure"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),  # Center title
    legend.position = "right",                              # Position legend on the right
    legend.title = element_text(size = 10, face = "bold"),  # Larger legend title
    legend.text = element_text(size = 8),                  # Adjust legend text size
    panel.grid = element_blank()                           # Remove gridlines for a cleaner look
  )

```

```{r}
# Plot the parallel trends
ggplot(df_wide_complete_no_na_hospital_closures, aes(x = Year, y = `Beschäftigungsquote (%)`, color = factor(treated_unit), group = treated_unit)) +
  stat_summary(fun = mean, geom = "line", linewidth = 1) +
  labs(
    title = "Parallel Trends Assumption Check",
    x = "Year",
    y = "Employment Rate (%)",
    color = "Treatment Received"
  ) +
  theme_minimal()

```


```{r}
df_wide_complete_no_na_hospital_closures_13_17 <- df_wide_complete_no_na_hospital_closures %>%
  mutate(
    sub_treated_group = ifelse(!is.na(year_of_closure), as.character(year_of_closure), "Untreated")
  ) %>%
  filter(sub_treated_group %in% c(2013, 2014, 2015, 2016, 2017) | sub_treated_group == "Untreated")

df_wide_complete_no_na_hospital_closures_13_17_mean <- df_wide_complete_no_na_hospital_closures_13_17 %>%
  group_by(Year, sub_treated_group) %>%
  summarise(
    mean_employment = mean(`Beschäftigungsquote (%)`, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(df_wide_complete_no_na_hospital_closures_13_17_mean, aes(x = Year, y = mean_employment, color = sub_treated_group, group = sub_treated_group)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  labs(
    title = "Employment Trends by Treatment Year",
    x = "Year",
    y = "Mean Employment Rate (%)",
    color = "Year of Closure"
  ) +
  theme_minimal()

ggplot(df_wide_complete_no_na_hospital_closures_13_17_mean, aes(x = Year, y = mean_employment, color = sub_treated_group, group = sub_treated_group)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  facet_wrap(~ sub_treated_group) +
  labs(
    title = "Employment Trends by Year of Closure",
    x = "Year",
    y = "Mean Employment Rate (%)",
    color = "Group",
    caption = "Each facet represents a treated group's year of closure. Untreated units are included for comparison."
  ) +
  theme_minimal()
```

```{r}
# Check for missing values in the relevant columns
df_wide_complete_no_na_hospital_closures_13_17 %>%
  summarise(
    missing_year = sum(is.na(Year)),
    missing_treatment = sum(is.na(treatment_received)),
    missing_employment = sum(is.na(`Beschäftigungsquote (%)`))
  )


df_wide_complete_no_na_hospital_closures_13_17 %>%
  group_by(Year, treatment_received) %>%
  summarise(
    mean_employment = mean(`Beschäftigungsquote (%)`, na.rm = TRUE), 
    .groups = "drop"
  ) %>%
  complete(Year, treatment_received, fill = list(mean_employment = NA))

df_wide_complete_no_na_hospital_closures_13_17 %>%
  group_by(Year, treatment_received) %>%
  summarise(count = n(), .groups = "drop")
```

```{r}
df_wide_complete_no_na_hospital_closures_13_17 <- df_wide_complete_no_na_hospital_closures_13_17 %>%
  rename(employment_level = "Beschäftigungsquote (%)") %>%
  rename(employment_level_female = "Frauenbeschäftigungsquote (%)")

# Run the DiD model
did_model <- lm(
  employment_level_female ~ treated_unit * treatment_received + factor(Year),
  data = df_wide_complete_no_na_hospital_closures_13_17
)

# View summary of results
summary(did_model)

```

```{r}
table(
  treated_unit = df_wide_complete_no_na_hospital_closures_13_17$treated_unit,
  treatment_received = df_wide_complete_no_na_hospital_closures_13_17$treatment_received
)

```

```{r}
did_model_simple <- lm(
  employment_level_female ~ treated_unit * treatment_received,
  data = df_wide_complete_no_na_hospital_closures_13_17
)

summary(did_model_simple)

```

```{r}
did_model_no_interaction <- lm(
  employment_level_female ~ treated_unit + treatment_received,
  data = df_wide_complete_no_na_hospital_closures_13_17
)

summary(did_model_no_interaction)

```


```{r}
did_model_with_year <- lm(
  employment_level_female ~ treated_unit + treatment_received + factor(Year),
  data = df_wide_complete_no_na_hospital_closures_13_17
)

summary(did_model_with_year)


```

```{r}

# Convert to panel data format
df_panel <- pdata.frame(df_wide_complete_no_na_hospital_closures_13_17, index = c("ARS_original", "Year"))

# Run fixed-effects model
did_model_fe <- plm(
  employment_level_female ~ treated_unit + treatment_received,
  data = df_panel,
  model = "within"
)

summary(did_model_fe)

```



```{r}
did_model_fe_with_year <- plm(
  employment_level_female ~ treated_unit + treatment_received + factor(Year),
  data = df_panel,
  model = "within"
)

summary(did_model_fe_with_year)

```

```{r}
# Interaction model for heterogeneous treatment effects
heterogeneous_model <- plm(
  employment_level_female ~ 
    treated_unit * Demografietyp + 
    treatment_received * Demografietyp + 
    factor(Year),
  data = df_panel,
  model = "within"
)

# Summary of the model
summary(heterogeneous_model)

```




```{r}


# Clustered standard errors
clustered_se <- vcovHC(did_model_fe_with_year, type = "HC1", cluster = "group")
coeftest(did_model_fe_with_year, vcov = clustered_se)

```

```{r}

event_study <- feols(
  employment_level_female ~ i(Year, treatment_received, ref = 2014) + factor(ARS_original),
  data = df_wide_complete_no_na_hospital_closures_13_17
)

summary(event_study)
```

```{r}
# Extract the coefficients for the interaction terms
event_coeff <- coef(event_study)[grep("Year::.*:treatment_received", names(coef(event_study)))]

# Extract confidence intervals for the same terms
conf_int <- confint(event_study)[grep("Year::.*:treatment_received", rownames(confint(event_study))), ]

# Prepare the data for plotting
event_data <- data.frame(
  year_relative = as.numeric(gsub("Year::", "", gsub(":treatment_received", "", names(event_coeff)))),
  estimate = event_coeff,
  lower = conf_int[, 1],
  upper = conf_int[, 2]
)


ggplot(event_data, aes(x = year_relative, y = estimate)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Event Study: Effect of Treatment on Employment Level",
    x = "Year Relative to Treatment",
    y = "Estimated Effect (with 95% CI)"
  ) +
  theme_minimal()

```

```{r}
df_wide_complete_no_na_hospital_closures_13_17 %>%
  group_by(ARS_original, Year) %>%
  summarise(count = n()) %>%
  filter(count > 1) %>%
  print()

```

```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

# df_wide_complete_no_na_hospital_closures_per_year_of_closure
df_wide_complete_no_na_hospital_closures_14_16_ <- df_wide_complete_no_na_hospital_closures_13_17 %>%
  filter(sub_treated_group %in% c(2014, 2015, 2016) | sub_treated_group == "Untreated") %>%
  group_by(ARS, Year) %>%
  summarise(
   employment_level = mean(employment_level, na.rm = TRUE),
  treatment_received = mean(as.numeric(treatment_received), na.rm = TRUE), # If needed for D
    demographic_type = Mode(Demografietyp)
  ) %>%
  ungroup() 

df_wide_complete_no_na_hospital_closures_13_17_ <- df_wide_complete_no_na_hospital_closures_13_17 %>%
  filter(sub_treated_group %in% c(2013, 2014, 2015, 2016, 2017) | sub_treated_group == "Untreated") %>%
  group_by(ARS, Year) %>%
  summarise(
    employment_level = mean(employment_level, na.rm = TRUE),
    treatment_received = mean(as.numeric(treatment_received), na.rm = TRUE), # If needed for D
    demographic_type = Mode(Demografietyp)
  ) %>%
  ungroup() 
```
### Incorporation PCA

```{r}
df_wide_complete_no_na_hospital_closures_13_17_test <- df_wide_complete_no_na_hospital_closures_14_16_

PCA_PVs_selected_columns_finance <- c(
    'Einzahlungen lfd. Verwaltung (Euro je Einwohner:in)',
    'Finanzeinzahlungen (Euro je Einwohner:in)',
    'Finanzsaldo (Euro je Einwohner:in)',
    'Grundsteuer B (Euro je Einwohner:in)',
    'Umlage an Gemeindeverbände (Euro je Einwohner:in)',
    'Verschuldung im Kernhaushalt (Euro je Einwohner:in)'
)

PCA_PVs_selected_columns_demographics <- c(
    'Anteil 65- bis 79-Jährige (%)',
    'Anteil Elternjahrgänge (%)',
    'Anteil unter 18-Jährige (%)',
    'Geburten (je 1.000 Einwohner:innen)',
    'Altenquotient (ab 65-Jährige je 100 Pers. der AG 20-64)',
    'Jugendquotient (unter 20-Jährige je 100 Pers. der AG 20-64)',
    'Gesamtquotient (unter 20-/ab 65-Jährige je 100 Pers. der AG 20-64)',
    'Natürlicher Saldo (je 1.000 Einwohner:innen)'
)

PCA_PVs_selected_columns_social <- c(
  'Einpersonen-Haushalte (%)',
  'Haushalte mit Kindern (%)',
  'Wohnfläche pro Person (m²)',
  'Wohnungen in Ein-/Zweifamilienhäusern (%)'
)

df_wide_complete_no_na_hospital_closures_13_17_finance_standardized_data <- scale(df_wide_complete_no_na_hospital_closures_13_17_test[, PCA_PVs_selected_columns_finance])

df_wide_complete_no_na_hospital_closures_13_17_demographics_standardized_data <- scale(df_wide_complete_no_na_hospital_closures_13_17_test[, PCA_PVs_selected_columns_demographics])

df_wide_complete_no_na_hospital_closures_13_17_social_standardized_data <- scale(df_wide_complete_no_na_hospital_closures_13_17_test[, PCA_PVs_selected_columns_social])

pca_result_finance <- prcomp(df_wide_complete_no_na_hospital_closures_13_17_finance_standardized_data, center = TRUE, scale = TRUE)

pca_result_demographics <- prcomp(df_wide_complete_no_na_hospital_closures_13_17_demographics_standardized_data, center = TRUE, scale = TRUE)

pca_result_social <- prcomp(df_wide_complete_no_na_hospital_closures_13_17_social_standardized_data, center = TRUE, scale = TRUE)

df_wide_complete_no_na_hospital_closures_13_17_test$demographics_PC1 <- pca_result_demographics$x[, 1]

df_wide_complete_no_na_hospital_closures_13_17_test$finance_PC1 <- pca_result_finance$x[, 1]

df_wide_complete_no_na_hospital_closures_13_17_test$social_PC1 <- pca_result_social$x[, 1]

# Extract the first principal component for each PCA
df_wide_complete_no_na_hospital_closures_13_17_test <- df_wide_complete_no_na_hospital_closures_13_17_test %>%
  mutate(
    pca_result_social = as.numeric(pca_result_social$x[, 1]),
    pca_result_finance = as.numeric(pca_result_finance$x[, 1]),
    pca_result_demographics = as.numeric(pca_result_demographics$x[, 1])
  )

```

```{r}
# Create the long-format dataframe for PCA columns
df_long <- df_wide_complete_no_na_hospital_closures_13_17_test %>%
  select(ARS, Year, pca_result_social, pca_result_finance, pca_result_demographics)

# Create matrices for each PCA component
social_matrix <- as.matrix(
  reshape2::dcast(df_long, ARS ~ Year, value.var = "pca_result_social")[, -1]
)

finance_matrix <- as.matrix(
  reshape2::dcast(df_long, ARS ~ Year, value.var = "pca_result_finance")[, -1]
)

demographics_matrix <- as.matrix(
  reshape2::dcast(df_long, ARS ~ Year, value.var = "pca_result_demographics")[, -1]
)

# Combine into a 3D covariate matrix
synth_X <- array(NA, dim = c(nrow(social_matrix), ncol(social_matrix), 3))
synth_X[, , 1] <- social_matrix
synth_X[, , 2] <- finance_matrix
synth_X[, , 3] <- demographics_matrix

# Check the dimensions of the resulting 3D array
dim(synth_X)

```

```{r}
dim(synth_Y_imputed)
dim(synth_Y)
dim(synth_X)
dim(synth_D)

```

```{r}
# Randomly sample treated and control units
set.seed(123)  # For reproducibility
treated_units <- which(rowSums(synth_D) > 0)
control_units <- which(rowSums(synth_D) == 0)

# Take a smaller sample
sampled_units <- c(sample(treated_units, min(10, length(treated_units))),
                   sample(control_units, min(100, length(control_units))))

# Subset the matrices
synth_Y_subset <- synth_Y[sampled_units, ]
synth_D_subset <- synth_D[sampled_units, ]
synth_X_subset <- synth_X[sampled_units, , , drop = FALSE]

```

```{r}
# Number of control units and pre-treatment periods in the subset
N0 <- sum(rowSums(synth_D) == 0)
T0 <- min(which(colSums(synth_D) > 0)) - 1

# Run SynthDiD on the subset
sdid <- synthdid::synthdid_estimate(
  Y = synth_Y_imputed,
  X = synth_X,
  N0 = N0,
  T0 = T0
)
```


```{r}
# Number of control units and pre-treatment periods in the subset
N0_subset <- sum(rowSums(synth_D_subset) == 0)
T0_subset <- min(which(colSums(synth_D_subset) > 0)) - 1

# Run SynthDiD on the subset
sdid_subset <- synthdid::synthdid_estimate(
  Y = synth_Y_subset,
  X = synth_X_subset,
  N0 = N0_subset,
  T0 = T0_subset
)
```

```{r}
# Extract treatment effect
treatment_effect <- as.numeric(sdid)

# Extract weights
weights <- attr(sdid, "weights")
lambda <- weights$lambda  # Time weights
omega <- weights$omega    # Unit weights

# Extract setup information
setup <- attr(sdid, "setup")
Y <- setup$Y  # Outcome matrix
N0 <- setup$N0  # Number of control units
T0 <- setup$T0  # Pre-treatment time steps

# Ensure rownames(Y) is not NULL; if it is, generate generic identifiers
if (is.null(rownames(Y))) {
  rownames(Y) <- paste0("Unit_", seq_len(nrow(Y)))
}

# Combine extracted information into data frames

# Summary of treatment effect and basic setup information
result_df <- data.frame(
  treatment_effect = treatment_effect,
  num_control_units = setup$N0,
  num_treated_units = nrow(setup$Y) - setup$N0,
  num_pre_treatment_periods = setup$T0,
  num_post_treatment_periods = ncol(setup$Y) - setup$T0
)

# Unit weights (omega) for control units
unit_weights_df <- data.frame(
  unit = rownames(Y)[1:length(omega)],  # Use only the relevant rows matching omega length
  omega = omega
)

# Time weights (lambda) for pre-treatment periods
time_weights_df <- data.frame(
  time = colnames(Y)[1:length(lambda)],  # Use only the relevant columns matching lambda length
  lambda = lambda
)

# View the data frames
print("Treatment Effect Summary:")
print(result_df)

print("Unit Weights:")
print(unit_weights_df)

print("Time Weights:")
print(time_weights_df)
```

```{r}
# Extract necessary information from the synthdid_estimate object
treatment_effect <- as.numeric(sdid)  # Extract the treatment effect
setup <- attr(sdid, "setup")  # Extract setup information
weights <- attr(sdid, "weights")  # Extract weights information

# Calculate the standard error (check if variance exists, otherwise fallback)
stderr <- if (!is.null(weights$variance)) {
  sqrt(weights$variance)
} else if (!is.null(weights$lambda)) {
  sqrt(mean((weights$lambda)^2)) / sqrt(length(weights$lambda))  # Fallback calculation
} else {
  NA  # If no valid fallback exists, return NA
}

# Calculate effective N0 and T0
effective_N0 <- sum(weights$omega > 0)  # Non-zero omega weights
effective_T0 <- sum(weights$lambda > 0)  # Non-zero lambda weights

# Generate the complete output
cat(sprintf(
  "Number of control units (N0): %d \nNumber of pre-treatment time steps (T0): %d \n",
  setup$N0, setup$T0
))
if (!is.na(stderr)) {
  cat(sprintf(
    "synthdid: %.3f +- %.3f. Effective N0/N0 = %.1f/%d~%.1f. Effective T0/T0 = %.1f/%d~%.1f. N1,T1 = %d,%d.\n",
    treatment_effect, stderr, effective_N0, setup$N0, effective_N0 / setup$N0,
    effective_T0, setup$T0, effective_T0 / setup$T0, setup$N1, setup$T1
  ))
} else {
  cat("Unable to compute standard error or additional summary statistics due to missing data.\n")
}


```
```{r}
cat(sprintf(
  "Number of control units (N0): %d \nNumber of pre-treatment time steps (T0): %d \n",
  setup$N0, setup$T0
))
cat(sprintf(
  "synthdid: %.3f +- %.3f. Effective N0/N0 = %.1f/%d~%.1f. Effective T0/T0 = %.1f/%d~%.1f. N1,T1 = %d,%d.\n",
  treatment_effect, stderr, effective_N0, setup$N0, effective_N0 / setup$N0,
  effective_T0, setup$T0, effective_T0 / setup$T0, setup$N1, setup$T1
))
cat("Lambda weights (time):", paste(round(weights$lambda, 3), collapse = ", "), "\n")
cat("Omega weights (units):", paste(round(weights$omega[1:100], 9), collapse = ", "), "... (truncated)\n") # Truncate for clarity
cat("Beta (covariates):", paste(round(weights$beta, 3), collapse = ", "), "\n")

```


```{r}
# Number of control units and pre-treatment time periods
N0 <- sum(rowSums(synth_D) == 0)  # Units that are never treated
T0 <- min(which(colSums(synth_D) > 0)) - 1  # First treatment year minus 1

# Run synthdid with PCA covariates
sdid_with_pca <- synthdid::synthdid_estimate(
  Y = synth_Y_imputed,
  X = synth_X,
  N0 = N0,
  T0 = T0
)
```

```{r}
sdid_with_pca
```

```{r}
# Get a summary of the object
summary(sdid_with_pca)

# Inspect the structure
str(sdid_with_pca)

```

```{r}
# Plot the results
synthdid::synthdid_plot(sdid_with_pca) +
  ggplot2::labs(
    title = "Synthetic Difference-in-Differences Estimate",
    x = "Time",
    y = "Outcome"
  )

```



```{r}
# Count the number of treated units per demographic type
df_wide_complete_no_na_hospital_closures_13_17_test %>%
  filter(treatment_received > 0) %>%  # Keep only treated units
  group_by(demographic_type) %>%
  summarise(
    treated_units = n_distinct(ARS)  # Count distinct ARS identifiers
  )

```


```{r}
# Filter the dataframe for the specific demographic type
df_wide_complete_no_na_hospital_closures_13_17_type_3 <- df_wide_complete_no_na_hospital_closures_13_17_ %>%
  filter(demographic_type == "Typ 3")
```


```{r}
df_wide_complete_no_na_hospital_closures_13_17_ %>%
  summarise(
    Year_NA = sum(is.na(Year)),
    ARS_NA = sum(is.na(ARS)),
    employment_level_NA = sum(is.na(employment_level))
  )

```


```{r}
synth_Y <- as.matrix(
  reshape2::dcast(
    df_wide_complete_no_na_hospital_closures_14_16_,
    ARS ~ Year,
    value.var = "employment_level"
  )[, -1]  # Remove the `ARS_original` column
)
dim(synth_Y)

```

```{r}
# Define a function to interpolate row-wise
interpolate_row <- function(row) {
  # Find the indices of non-NA values
  non_na_indices <- which(!is.na(row))
  
  # If there are no non-NA values, return the row as-is
  if (length(non_na_indices) == 0) return(row)
  
  # Loop through each element of the row
  for (i in seq_along(row)) {
    if (is.na(row[i])) {
      # Find the nearest non-NA values before and after
      before <- max(non_na_indices[non_na_indices < i], na.rm = TRUE)
      after <- min(non_na_indices[non_na_indices > i], na.rm = TRUE)
      
      # Replace NA with the average of before and after
      if (!is.na(before) & !is.na(after)) {
        row[i] <- mean(c(row[before], row[after]), na.rm = TRUE)
      } else if (!is.na(before)) {
        row[i] <- row[before]
      } else if (!is.na(after)) {
        row[i] <- row[after]
      }
    }
  }
  return(row)
}

# Apply the function to each row of the matrix
synth_Y_imputed <- t(apply(synth_Y, 1, interpolate_row))
dim(synth_Y)
dim(synth_Y_imputed)
dim(synth_X)

```
```{r}
# Check if there are any NA values in synth_Y_imputed
any_na <- any(is.na(synth_D_imputed))

# Print the result
if (any_na) {
  cat("The matrix synth_Y_imputed still contains NA values.\n")
} else {
  cat("The matrix synth_Y_imputed has no NA values.\n")
}

```

```{r}
# Ensure treatment_received is binary
df_wide_complete_no_na_hospital_closures_14_16_$treatment_received <- 
  ifelse(df_wide_complete_no_na_hospital_closures_14_16_$treatment_received > 0, 1, 0)

# Create the treatment matrix
synth_D <- as.matrix(
  reshape2::dcast(
    df_wide_complete_no_na_hospital_closures_14_16_,
    ARS ~ Year,
    value.var = "treatment_received",
    fun.aggregate = max  # Use max to ensure binary values
  )[, -1]  # Remove the `ARS` column
)

table(as.vector(synth_D))
table(df_wide_complete_no_na_hospital_closures_14_16_$treatment_received)

# Replace -Inf values with the next available value on the right, or the last available value on the left
replace_with_next_or_previous <- function(row) {
  for (i in seq_along(row)) {
    if (is.infinite(row[i]) && row[i] == -Inf) {
      # Try to find the next available value on the right
      next_val <- row[(i + 1):length(row)][!is.infinite(row[(i + 1):length(row)]) & !is.na(row[(i + 1):length(row)])]
      
      # If no next value, use the last available value on the left
      if (length(next_val) > 0) {
        row[i] <- next_val[1]
      } else {
        prev_val <- row[1:(i - 1)][!is.infinite(row[1:(i - 1)]) & !is.na(row[1:(i - 1)])]
        if (length(prev_val) > 0) {
          row[i] <- prev_val[length(prev_val)]  # Take the last available value on the left
        }
      }
    }
  }
  return(row)
}

# Apply the function row-wise to the matrix
synth_D <- t(apply(synth_D, 1, replace_with_next_or_previous))

# Validate the resulting matrix
table(as.vector(synth_D))
```

#Covariates 
What would cause different units to react differently to the treatment? (differences within treatment group, moderating variables)
- next hospital (as subsequent employer)
- size closing hospital (as an employer)
- skill level working force
- alternative employers (urbanization, economic well-being of region)

What could trigger treatment in units that distinguishes them systematically from untreated units? (confounding variables)
- Economic Baseline Conditions

- Moving trimmed average 
-- to include time-variant variables that have an effect on the effect of the treatment but are not affected/unlikely to be affected by the treatment
--- demographic type
- baseline comparison (values from 2006/07/08)
-- to compare locations on stable context-variables
--- Demographic Factors
---- "population density" as an indicator of urbanization
---- "Altenquotient" as an indicator of age structure (does that make sense? What is the assumed mechanism here?)
---- "Jugendquotient" as an indicator of age structure (does that make sense? What is the assumed mechanism here?)
--- Pre-treatment Economic Baseline Conditions
---- "Einkommenssteuer" as an indicator of economic well-being
---- "Verschuldung im Kernhaushalt (Euro je Einwohner:in)" as an indicator of economic well-being
-- variables affected by treatment (for instance, as mediators)
--- population variables that might react to treatment but also have an effect on effect size
---- Hochqualifizierte am Wohnort
 